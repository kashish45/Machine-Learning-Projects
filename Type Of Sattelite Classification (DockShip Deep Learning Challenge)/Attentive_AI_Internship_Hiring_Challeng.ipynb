{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attentive AI Internship Hiring Challeng",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5W8w_tLnOWtu",
        "outputId": "7c425b1d-b529-4b03-9f4b-fa3b4f7d0688"
      },
      "source": [
        "!wget -O \"attentive_ai_internship_hiring_challenge-dataset.zip\" \"https://dockship-job-models.s3.ap-south-1.amazonaws.com/c452513e7cb7c4db308401f0f0079e51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIDOPTEUZ2LEOQEGQ%2F20210124%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20210124T070931Z&X-Amz-Expires=1800&X-Amz-Signature=5667dd3f3669e867dcd61a5a1ad215b4034ccf585b7aabe1f2ec65a4d8fd3244&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22attentive_ai_internship_hiring_challenge-dataset.zip%22\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-24 07:18:20--  https://dockship-job-models.s3.ap-south-1.amazonaws.com/c452513e7cb7c4db308401f0f0079e51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIDOPTEUZ2LEOQEGQ%2F20210124%2Fap-south-1%2Fs3%2Faws4_request&X-Amz-Date=20210124T070931Z&X-Amz-Expires=1800&X-Amz-Signature=5667dd3f3669e867dcd61a5a1ad215b4034ccf585b7aabe1f2ec65a4d8fd3244&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22attentive_ai_internship_hiring_challenge-dataset.zip%22\n",
            "Resolving dockship-job-models.s3.ap-south-1.amazonaws.com (dockship-job-models.s3.ap-south-1.amazonaws.com)... 52.219.64.87\n",
            "Connecting to dockship-job-models.s3.ap-south-1.amazonaws.com (dockship-job-models.s3.ap-south-1.amazonaws.com)|52.219.64.87|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1417273382 (1.3G) [binary/octet-stream]\n",
            "Saving to: ‘attentive_ai_internship_hiring_challenge-dataset.zip’\n",
            "\n",
            "attentive_ai_intern 100%[===================>]   1.32G  14.6MB/s    in 96s     \n",
            "\n",
            "2021-01-24 07:19:57 (14.1 MB/s) - ‘attentive_ai_internship_hiring_challenge-dataset.zip’ saved [1417273382/1417273382]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "842m7aUrObk-"
      },
      "source": [
        "#importing libraries\r\n",
        "import pandas as pd\r\n",
        "from tensorflow.keras.optimizers import RMSprop\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\r\n",
        "from keras.models import Sequential\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import Model\r\n",
        "from keras import Model\r\n",
        "from os import getcwd\r\n",
        "from keras.layers import Dense, Activation, BatchNormalization\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NzWv0OGPE2V"
      },
      "source": [
        "!unzip -q \"attentive_ai_internship_hiring_challenge-dataset.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8c-eRjBPHMt"
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint,ReduceLROnPlateau\r\n",
        "\r\n",
        "\r\n",
        "earlyStopping = EarlyStopping(monitor='val_loss', patience = 7, verbose=0, mode='min')\r\n",
        "mcp_save_xc = ModelCheckpoint(filepath='mdl_wts_xc.hdf5', save_best_only=True, monitor='val_accuracy', mode='max')\r\n",
        "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_delta=1e-4, mode='min')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ien8FgeePLYg"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/merged_data/train_challenge.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "yWtV7LaHPMvU",
        "outputId": "e3241c52-9dcb-4af8-e1a9-c6bfefafb59a"
      },
      "source": [
        "train_df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.jpg</td>\n",
              "      <td>Adhered</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.jpg</td>\n",
              "      <td>Adhered</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0        1\n",
              "0  0.jpg  Adhered\n",
              "1  1.jpg  Adhered"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvLXGsNIPNz2",
        "outputId": "49c4f807-d12e-4d80-f075-0a4b07035a67"
      },
      "source": [
        "train_df['1'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Steel               2322\n",
              "Adhered             2321\n",
              "Concrete            1146\n",
              "Shingle             1072\n",
              "Plastic & fabric    1068\n",
              "Ballasted            715\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8XHOXNJPPm9",
        "outputId": "2c434036-10ab-414b-cc7c-555a1061706a"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "TRAINING_DIR = \"/content/merged_data/train\"\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1/255,\r\n",
        "    rotation_range=40,\r\n",
        "    width_shift_range=.2,\r\n",
        "    height_shift_range=.2,\r\n",
        "    shear_range=.2,\r\n",
        "    validation_split=0.05,\r\n",
        "    zoom_range=.2,\r\n",
        "    horizontal_flip=True,\r\n",
        "    fill_mode='nearest'\r\n",
        ")\r\n",
        "train_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\r\n",
        "    directory = TRAINING_DIR,\r\n",
        "    x_col=\"0\",\r\n",
        "    y_col=\"1\",\r\n",
        "    batch_size = 64,\r\n",
        "    subset = 'training',\r\n",
        "    class_mode='categorical',\r\n",
        "    target_size=(150, 150)\r\n",
        ")\r\n",
        "\r\n",
        "validation_generator = train_datagen.flow_from_dataframe(dataframe=train_df,\r\n",
        "    directory = TRAINING_DIR,\r\n",
        "    x_col=\"0\",\r\n",
        "    y_col=\"1\",\r\n",
        "    batch_size = 64,\r\n",
        "    subset = 'validation',\r\n",
        "    class_mode='categorical',\r\n",
        "    target_size=(150,150)\r\n",
        ")   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8212 validated image filenames belonging to 6 classes.\n",
            "Found 432 validated image filenames belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7VFLtiwPX7s"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\r\n",
        "#from keras.applications.vgg16 import VGG16\r\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\r\n",
        "from tensorflow.keras.preprocessing import image\r\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "import numpy as np\r\n",
        "from glob import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3HTUXSyPe-1"
      },
      "source": [
        "IMAGE_SIZE = [150, 150]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1tmVIdmPjEI",
        "outputId": "5e55cd9b-b3ea-47ae-cd15-3f3c7eb932ec"
      },
      "source": [
        "resnet = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIxx-g3IPl2O"
      },
      "source": [
        "for layer in resnet.layers:\r\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwQr3TY8Pn5S"
      },
      "source": [
        "b = Flatten()(resnet.output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0etMBCtrPrCn"
      },
      "source": [
        "\r\n",
        "prediction = Dense(6, activation='softmax')(b)\r\n",
        "\r\n",
        "# create a model object\r\n",
        "model = Model(inputs=resnet.input, outputs=prediction)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQdoxHyNQFku"
      },
      "source": [
        "model.compile(\r\n",
        "  loss='categorical_crossentropy',\r\n",
        "  optimizer='adam',\r\n",
        "  metrics=['accuracy']\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7Zspyc3Pzdi",
        "outputId": "11cb5403-f8cb-4f3a-d4ca-6c853b913d8a"
      },
      "source": [
        "history_res = model.fit(\r\n",
        "      train_generator,  \r\n",
        "      callbacks=[ mcp_save_xc, reduce_lr_loss,earlyStopping],\r\n",
        "      validation_data = validation_generator,\r\n",
        "      epochs = 40,\r\n",
        "      verbose = 1,\r\n",
        " )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "129/129 [==============================] - 137s 998ms/step - loss: 2.0492 - accuracy: 0.2529 - val_loss: 1.5739 - val_accuracy: 0.3264\n",
            "Epoch 2/40\n",
            "129/129 [==============================] - 128s 990ms/step - loss: 1.6779 - accuracy: 0.3090 - val_loss: 1.3539 - val_accuracy: 0.5347\n",
            "Epoch 3/40\n",
            "129/129 [==============================] - 127s 987ms/step - loss: 1.7006 - accuracy: 0.3171 - val_loss: 1.4306 - val_accuracy: 0.5579\n",
            "Epoch 4/40\n",
            "129/129 [==============================] - 127s 979ms/step - loss: 1.6173 - accuracy: 0.3283 - val_loss: 1.5846 - val_accuracy: 0.2940\n",
            "Epoch 5/40\n",
            "129/129 [==============================] - 126s 978ms/step - loss: 1.5835 - accuracy: 0.3492 - val_loss: 1.5918 - val_accuracy: 0.3426\n",
            "Epoch 6/40\n",
            "129/129 [==============================] - 126s 978ms/step - loss: 1.6197 - accuracy: 0.3381 - val_loss: 1.3772 - val_accuracy: 0.5648\n",
            "Epoch 7/40\n",
            "129/129 [==============================] - 126s 978ms/step - loss: 1.5637 - accuracy: 0.3569 - val_loss: 1.5220 - val_accuracy: 0.3750\n",
            "\n",
            "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "Epoch 8/40\n",
            " 38/129 [=======>......................] - ETA: 1:29 - loss: 1.5305 - accuracy: 0.3558"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_sN_-7FQBRw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}